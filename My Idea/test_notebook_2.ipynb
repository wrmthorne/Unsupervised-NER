{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from kmeans_pytorch import kmeans, kmeans_predict\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "The CoNLL2003 dataset is first used to evaluate performance. The ground truth ner tags are used only select named entities to allow the class allocation to be tested independently by assumung perfect extraction. Later work will be conducted into identifying which entities should be labelled. NER tags are mapped to a non-BIO based scheme to reduce the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8663894b84a4eb6a37f9356d10d6176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d68aabf815840fab5f888718d7eee2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/14041 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32fded5dfc72400992fecba6a64f92c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3250 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "208fa52e3f79482b85bd52d0720997c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3453 [00:00<?, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'ner_tags': [2, 0, 4, 0, 0, 0, 4, 0, 0], 'input_ids': [[101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102]], 'target_mask': [[0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "original_tags = {'O': 0, 'B-PER': 1, 'I-PER': 2, 'B-ORG': 3, 'I-ORG': 4, 'B-LOC': 5, 'I-LOC': 6, 'B-MISC': 7, 'I-MISC': 8}\n",
    "# New non-BIO scheme\n",
    "new_tags = {0: 0, 1: 1, 2: 1, 3: 2, 4: 2, 5: 3, 6: 3, 7: 4, 8: 4}\n",
    "new_tags_string = {0: 'O', 1: 'PER', 2: 'ORG', 3: 'LOC', 4: 'MISC'}\n",
    "\n",
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "def process_batch(example):\n",
    "    target_mask = torch.tensor([0], dtype=torch.int8)\n",
    "    input_ids = torch.tensor([101], dtype=int)\n",
    "\n",
    "    for token, ner_tag in zip(example['tokens'], example['ner_tags']):\n",
    "        encoded_tok = tokenizer.encode(token, return_tensors='pt')[0, 1:-1]\n",
    "\n",
    "        input_ids = torch.cat((input_ids, encoded_tok))\n",
    "        target_mask = torch.cat((target_mask, torch.full_like(encoded_tok, 1 if ner_tag != 0 else 0)))\n",
    "\n",
    "    target_mask = torch.cat((target_mask, torch.tensor([0]))).unsqueeze(0)\n",
    "    input_ids = torch.cat((input_ids, torch.tensor([102]))).unsqueeze(0)\n",
    "\n",
    "    return {'input_ids': input_ids, 'target_mask': target_mask}\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = dataset[split].map(lambda batch: {\n",
    "        **process_batch(batch),\n",
    "        'ner_tags': [new_tags[tag] for tag in batch['ner_tags']]\n",
    "        }).remove_columns(['id', 'chunk_tags'])\n",
    "\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Define a method to extract the context vectors of the extracted entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "def get_context_vectors(input_ids, attention_mask, target_mask):\n",
    "    # Ignore gradient to improve performance\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.to(device), attention_mask.to(device))\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "    # Extract model embeddings layer activations\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove batches dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap layer and token dimensions\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "    \n",
    "    # Identify indices within encoded text to calculate context embeddings\n",
    "    target_indices = target_mask.to(device).nonzero()\n",
    "\n",
    "    # Use the sum of the last 4 embedding layers as an aggregation of context for the selected indices\n",
    "    stacked_token_embeddings = token_embeddings.repeat(torch.sum(target_mask), 1, 1, 1)\n",
    "    embedding_aggregate = torch.sum(stacked_token_embeddings[torch.arange(0, torch.sum(target_mask)), target_indices[:, 1], -4:], dim=1)\n",
    "\n",
    "    return embedding_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all of the context vectors using the defined method on the training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Contexts: 100%|██████████| 14041/14041 [02:38<00:00, 88.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65030, 768])\n",
      "65030\n"
     ]
    }
   ],
   "source": [
    "context_vectors = torch.empty(0, 768).to(device)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "for example in tqdm(dataset['train'], desc='Collecting Contexts'):\n",
    "    input_ids = torch.tensor(example['input_ids'])\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    target_mask = torch.tensor(example['target_mask'])\n",
    "\n",
    "    embedding_aggregate = get_context_vectors(input_ids, attention_mask, target_mask)\n",
    "    context_vectors = torch.cat((context_vectors, embedding_aggregate))\n",
    "\n",
    "print(context_vectors.shape)\n",
    "print(sum(sum(x[0]) for x in dataset['train']['target_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster\n",
    "\n",
    "Cluster context vectors using kmeans with $k = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 31it [00:08,  3.56it/s, center_shift=0.000071, iteration=31, tol=0.000100]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65030, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_ids, cluster_centres = kmeans(context_vectors, 4, distance='cosine', device=device)\n",
    "\n",
    "print(context_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually evaluate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n",
      "{'European': [0], 'Commission': [1], 'German': [0], 'British': [1]}\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "example = dataset['train'][i]\n",
    "\n",
    "input_ids = torch.tensor(example['input_ids'])\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "target_mask = torch.tensor(example['target_mask'])\n",
    "\n",
    "test_context_vectors = get_context_vectors(input_ids, attention_mask, target_mask)\n",
    "\n",
    "if len(test_context_vectors) == 1:\n",
    "    test_context_vectors = test_context_vectors.repeat(2, 1)\n",
    "\n",
    "test_cluster_ids = kmeans_predict(test_context_vectors, cluster_centres, device=device)\n",
    "test_clusters_ids_list = test_cluster_ids.squeeze().tolist()\n",
    "\n",
    "token_ids = {token: tokenizer.encode(token)[1:-1] for token, ner_tag in zip(example['tokens'], example['ner_tags']) if ner_tag != 0}\n",
    "token_predictions = {token: [test_clusters_ids_list.pop(0) for _ in range(len(ids))] for token, ids in token_ids.items()}\n",
    "\n",
    "print(token_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Calculate context vectors for all validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Contexts: 100%|██████████| 3250/3250 [00:37<00:00, 87.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16225, 768])\n",
      "16225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_context_vectors = torch.empty(0, 768).to(device)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "for example in tqdm(dataset['validation'], desc='Collecting Contexts'):\n",
    "    input_ids = torch.tensor(example['input_ids'])\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    target_mask = torch.tensor(example['target_mask'])\n",
    "\n",
    "    embedding_aggregate = get_context_vectors(input_ids, attention_mask, target_mask)\n",
    "    val_context_vectors = torch.cat((val_context_vectors, embedding_aggregate))\n",
    "\n",
    "print(val_context_vectors.shape)\n",
    "print(sum(sum(x[0]) for x in dataset['validation']['target_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify cluster allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n"
     ]
    }
   ],
   "source": [
    "val_cluster_ids = kmeans_predict(val_context_vectors, cluster_centres, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign cluster ids to each token in the dataset and obtain a single cluster id for items with multiple, i.e. words composed of sub-word level tokens, by taking the mode or majority vote. Using `max(set(lst), key=lst.count))` for majority voting yields the most frequent list item or, in the case that there is a tie for most frequent, the item with the lowest value:\n",
    "\n",
    "```python\n",
    "a = [1, 1, 1, 0, 0]\n",
    "b = [1, 1, 0, 0]\n",
    "max(set(a), key=a.count)\n",
    "# >> 1\n",
    "max(set(b), key=b.count)\n",
    "# >> 0\n",
    "```\n",
    "Another implementation may find that the head sub-word token is more informative.\n",
    "\n",
    "Also collect the ground truth NER tag for each entity for comparison. `Note`: The cluster id numbers will not correspond to the NER tag numbers. They are two sets of labels that need to be mapped to eachother in some way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [00:01<00:00, 2188.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'2-LEICESTERSHIRE': {'cluster_id': 3, 'ner_tag': 2}},\n",
      " {'0-LONDON': {'cluster_id': 3, 'ner_tag': 3}},\n",
      " {'0-West': {'cluster_id': 0, 'ner_tag': 4},\n",
      "  '1-Indian': {'cluster_id': 0, 'ner_tag': 4},\n",
      "  '12-Leicestershire': {'cluster_id': 1, 'ner_tag': 2},\n",
      "  '14-Somerset': {'cluster_id': 1, 'ner_tag': 2},\n",
      "  '3-Phil': {'cluster_id': 1, 'ner_tag': 1},\n",
      "  '4-Simmons': {'cluster_id': 1, 'ner_tag': 1}}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_clusters_ids_list = val_cluster_ids.squeeze().tolist()\n",
    "sample_predictions = []\n",
    "\n",
    "for example in tqdm(dataset['validation']):\n",
    "    token_ids = {f'{i}-{token}': {\n",
    "        'ids': tokenizer.encode(token)[1:-1],\n",
    "        'ner_tag': ner_tag\n",
    "     } for i, (token, ner_tag) in enumerate(zip(example['tokens'], example['ner_tags'])) if ner_tag != 0}\n",
    "\n",
    "    token_predictions = {token: {\n",
    "        'cluster_id': max(set((lst := [val_clusters_ids_list.pop(0) for _ in range(len(attributes['ids']))])), key=lst.count),\n",
    "        'ner_tag': attributes['ner_tag']\n",
    "     } for token, attributes in token_ids.items()}\n",
    "\n",
    "    sample_predictions.append(token_predictions)\n",
    "\n",
    "pprint(sample_predictions[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find which mapping of cluster ids to NER ids yields the best f-1 performance. `Note`: This calculation does not consider entities that weren't tagged by using PROPN. This is to control for the initial entity extraction task which will be reviewed at another stage. This is to just evaluate whether this style of clustering is suitable for identifying entity classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Distribution: Counter({1: 4159, 0: 2216, 2: 1520, 3: 708})\n",
      "NER Tag Distribution: Counter({1: 3149, 3: 2094, 2: 2092, 4: 1268})\n",
      "defaultdict(<class 'dict'>, {3: {2: 189, 3: 346, 4: 142, 1: 31}, 0: {4: 629, 3: 1366, 2: 215, 1: 6}, 1: {1: 1848, 2: 1567, 3: 280, 4: 464}, 2: {1: 1264, 3: 102, 2: 121, 4: 33}})\n",
      "Counter({(1, 1): 1848, (1, 2): 1567, (0, 3): 1366, (2, 1): 1264, (0, 4): 629, (1, 4): 464, (3, 3): 346, (1, 3): 280, (0, 2): 215, (3, 2): 189, (3, 4): 142, (2, 2): 121, (2, 3): 102, (2, 4): 33, (3, 1): 31, (0, 1): 6})\n",
      "28.384476534296027\n",
      "28.384476534296027\n",
      "28.384476534296027\n",
      "28.384476534296027\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter, deque, defaultdict\n",
    "\n",
    "cluster_counts = Counter([x_i['cluster_id'] for x in sample_predictions for x_i in x.values()])\n",
    "ner_counts = Counter([x_i['ner_tag'] for x in sample_predictions for x_i in x.values()])\n",
    "\n",
    "print(f'Cluster Distribution: {cluster_counts}')\n",
    "print(f'NER Tag Distribution: {ner_counts}')\n",
    "\n",
    "mapping_freqs = Counter([(x_i['cluster_id'], x_i['ner_tag']) for x in sample_predictions for x_i in x.values()])\n",
    "p = defaultdict(dict)\n",
    "\n",
    "for k, v in mapping_freqs.items():\n",
    "    p[k[0]].update({k[1]: v})\n",
    "\n",
    "print(p)\n",
    "\n",
    "print(mapping_freqs)\n",
    "\n",
    "a = [0, 1, 2, 3]\n",
    "b = deque([1, 2, 3, 4])\n",
    "\n",
    "for i in range(len(a)):\n",
    "    b.rotate(i)\n",
    "    mapping = {a_i: b_i for a_i, b_i in zip(a, b)}\n",
    "\n",
    "    print(p[0][4] / sum([p[0][x] for x in p[0].keys() if x != p[0][4]]) * 100)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('gatenlp_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5e1f5b7c86f3a3638279a01f4f037ad1bf683427fdb20ed40b35537abd7abab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
