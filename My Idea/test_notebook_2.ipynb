{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel, BertTokenizer, BertModel\n",
    "from kmeans_pytorch import kmeans, kmeans_predict\n",
    "import json\n",
    "from pprint import pprint\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import gc\n",
    "from datasets import load_dataset\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "\n",
    "# tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "# model = RobertaModel.from_pretrained('roberta-base', output_hidden_states=True)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "18103156e6e54f7a9b06a83c0444fd09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-219dc2dcd710c718.arrow\n",
      "Loading cached processed dataset at /home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-8ec9299742aa5129.arrow\n",
      "Loading cached processed dataset at /home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-14e3409507fc4dc2.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0], 'text': 'EU rejects German call to boycott British lamb.', 'encoded_PROPNS': [7270]}\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"conll2003\")\n",
    "\n",
    "for split in ['train', 'validation', 'test']:\n",
    "    dataset[split] = dataset[split].map(lambda batch: {\n",
    "        'text': TreebankWordDetokenizer().detokenize(batch['tokens']),\n",
    "        'encoded_PROPNS': [ids for tok, pos_tag in zip(batch['tokens'], batch['pos_tags']) for ids in tokenizer.encode(tok)[1:-1] if pos_tag in [22, 23]]\n",
    "        }).remove_columns(['id', 'chunk_tags'])\n",
    "\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = dataset['train']['text']\n",
    "encoded_PROPNS = dataset['train']['encoded_PROPNS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context_vectors(sentence, encoded_ids):\n",
    "    enc_sent = tokenizer.encode_plus(sentence, return_tensors='pt')\n",
    "    ids_t = torch.tensor(encoded_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(enc_sent.input_ids.to(device), enc_sent.attention_mask.to(device))\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Extract model embeddings layer activations\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove batches dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap layer and token dimensions\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "    \n",
    "    # Identify indices within encoded text to calculate context embeddings\n",
    "    target_indices = (enc_sent.input_ids.T == ids_t).nonzero()\n",
    "\n",
    "    # Use the sum of the last 4 embedding layers as an aggregation of context for the selected indices\n",
    "    stacked_token_embeddings = token_embeddings.repeat(ids_t.size(0), 1, 1, 1)\n",
    "    embedding_aggregate = torch.sum(stacked_token_embeddings[target_indices[:, 1], target_indices[:, 0], -4:], dim=1)\n",
    "\n",
    "    print(embedding_aggregate.shape)\n",
    "\n",
    "    return embedding_aggregate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14041it [02:34, 91.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75289, 768])\n",
      "66214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.eval()\n",
    "context_vectors = torch.empty(0, 768).to(device)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for sentence, encoded_ids in tqdm(zip(texts, encoded_PROPNS)):\n",
    "\n",
    "    enc_sent = tokenizer.encode_plus(sentence, return_tensors='pt')\n",
    "    ids_t = torch.tensor(encoded_ids)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(enc_sent.input_ids.to(device), enc_sent.attention_mask.to(device))\n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    # Extract model embeddings layer activations\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove batches dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap layer and token dimensions\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "    \n",
    "    # Identify indices within encoded text to calculate context embeddings\n",
    "    target_indices = (enc_sent.input_ids.T == ids_t).nonzero()\n",
    "\n",
    "    # Use the sum of the last 4 embedding layers as an aggregation of context for the selected indices\n",
    "    stacked_token_embeddings = token_embeddings.repeat(ids_t.size(0), 1, 1, 1)\n",
    "    embedding_aggregate = torch.sum(stacked_token_embeddings[target_indices[:, 1], target_indices[:, 0], -4:], dim=1)\n",
    "\n",
    "    context_vectors = torch.cat((context_vectors, embedding_aggregate))\n",
    "\n",
    "print(context_vectors.shape)\n",
    "print(len([i for id in dataset['train']['encoded_PROPNS'] for i in id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster\n",
    "\n",
    "Cluster context vectors using kmeans with $k = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 49it [00:15,  3.25it/s, center_shift=0.000062, iteration=49, tol=0.000100]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([75289, 768])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "cluster_ids, cluster_centres = kmeans(context_vectors, 4, distance='cosine', device=device)\n",
    "\n",
    "print(context_vectors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually inspect results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 768])\n",
      "predicting on cuda..\n",
      "[('President', [1]), ('Hafez', [3, 3, 3]), ('Israel', [1]), ('Foreign', [1]), ('Minister', [1]), ('David', [1]), ('Levy', [3]), ('Israel', [3]), ('Radio', [1])]\n"
     ]
    }
   ],
   "source": [
    "i = 106\n",
    "text = dataset['train'][i]['text']\n",
    "encoded_ids = dataset['train'][i]['encoded_PROPNS']\n",
    "\n",
    "embedding_aggregate = get_context_vectors(text, encoded_ids)\n",
    "\n",
    "if embedding_aggregate.size(0) == 1:\n",
    "    embedding_aggregate = embedding_aggregate.repeat(2, 1)\n",
    "\n",
    "clusters = kmeans_predict(embedding_aggregate, cluster_centres, device=device, distance='cosine')\n",
    "\n",
    "encoded_sent = tokenizer.encode(text)\n",
    "encoded_tokens = [(token, tokenizer.encode(token)[1:-1]) for token, pos_tag in zip(dataset['train'][i]['tokens'], dataset['train'][i]['pos_tags']) if pos_tag in [22, 23]]\n",
    "clusters_list = clusters.tolist()\n",
    "\n",
    "thing = [(enc_toks[0], [clusters_list.pop(0) for _ in range(len(enc_toks[1]))]) for enc_toks in encoded_tokens]\n",
    "print(thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('gatenlp_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed9bce70e2f85c08309b76a92bdcd846293a0270c4e50df8d2f5fae8e918b159"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
