{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised NER\n",
    "\n",
    "This is the development notebook of an unsupervised algorithm for named entity recognition using BERT context vectors. The general idea is to tag potential named entities in text using some other algorithm such as POS tagging and then classify them using BERT's context vectors for the tagged terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from kmeans_pytorch import kmeans, kmeans_predict\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "The CoNLL2003 dataset is first used to evaluate performance. The ground truth ner tags are used only select named entities to allow the class allocation to be tested independently by assumung perfect extraction. Later work will be conducted into identifying which entities should be labelled. NER tags are mapped to a non-BIO based scheme to reduce the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120.68s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 137.15it/s]\n",
      "Loading cached processed dataset at /home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-4090c730078341bd.arrow\n",
      "Loading cached processed dataset at /home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-6a0d385219cba2b7.arrow\n",
      "Loading cached processed dataset at /home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-7dd3c7e92e5c2986.arrow\n",
      "{'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'ner_tags': [2, 0, 4, 0, 0, 0, 4, 0, 0], 'input_ids': [[101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102]], 'target_mask': [[0, 1, 0, 2, 0, 0, 0, 3, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "!python format_conll2003.py --bert_model $model_name\n",
    "\n",
    "with open('./data/CoNLL2003/tag_mappings.json') as file:\n",
    "    tag_mappings = json.load(file)\n",
    "\n",
    "dataset = load_from_disk('./data/CoNLL2003')\n",
    "\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Define a method to extract the context vectors of the extracted entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "def get_context_vectors(input_ids, attention_mask, target_mask):\n",
    "    # Ignore gradient to improve performance\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.to(device), attention_mask.to(device))\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "    # Extract model embeddings layer activations\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove batches dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap layer and token dimensions\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "    \n",
    "    # Identify indices within encoded text to calculate context embeddings\n",
    "    target_indices = target_mask.to(device).nonzero()\n",
    "\n",
    "    # Use the sum of the last 4 embedding layers as an aggregation of context for the selected indices\n",
    "    stacked_token_embeddings = token_embeddings.repeat(torch.sum(target_mask), 1, 1, 1)\n",
    "    embedding_aggregate = torch.sum(stacked_token_embeddings[torch.arange(0, torch.sum(target_mask)), target_indices[:, 1], -4:], dim=1)\n",
    "\n",
    "    return embedding_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all of the context vectors using the defined method on the training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Contexts: 100%|██████████| 14041/14041 [02:41<00:00, 87.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65030, 768])\n",
      "65030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context_vectors = torch.empty(0, 768).to(device)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "for example in tqdm(dataset['train'], desc='Collecting Contexts'):\n",
    "    input_ids = torch.tensor(example['input_ids'])\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    target_mask = torch.tensor(example['target_mask'])\n",
    "\n",
    "    embedding_aggregate = get_context_vectors(input_ids, attention_mask, target_mask)\n",
    "    context_vectors = torch.cat((context_vectors, embedding_aggregate))\n",
    "\n",
    "print(context_vectors.shape)\n",
    "print(sum(sum(x[0]) for x in dataset['train']['target_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster\n",
    "\n",
    "Cluster context vectors using kmeans with $k = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 39it [00:11,  3.33it/s, center_shift=0.000000, iteration=39, tol=0.000100]  \n"
     ]
    }
   ],
   "source": [
    "# Seeds for reproducing cluster centres\n",
    "torch.random.seed = 42\n",
    "torch.cuda.seed = 42\n",
    "np.random.seed(42)\n",
    "\n",
    "distance = 'cosine' # 'euclidean'\n",
    "\n",
    "cluster_ids, cluster_centres = kmeans(context_vectors, 4, distance=distance, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually evaluate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n",
      "{'European': [3], 'Commission': [3], 'German': [3], 'British': [3]}\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "example = dataset['train'][i]\n",
    "\n",
    "input_ids = torch.tensor(example['input_ids'])\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "target_mask = torch.tensor(example['target_mask'])\n",
    "\n",
    "test_context_vectors = get_context_vectors(input_ids, attention_mask, target_mask)\n",
    "\n",
    "if len(test_context_vectors) == 1:\n",
    "    test_context_vectors = test_context_vectors.repeat(2, 1)\n",
    "\n",
    "test_cluster_ids = kmeans_predict(test_context_vectors, cluster_centres, distance=distance, device=device)\n",
    "test_clusters_ids_list = test_cluster_ids.squeeze().tolist()\n",
    "\n",
    "token_ids = {token: tokenizer.encode(token)[1:-1] for token, ner_tag in zip(example['tokens'], example['ner_tags']) if ner_tag != 0}\n",
    "token_predictions = {token: [test_clusters_ids_list.pop(0) for _ in range(len(ids))] for token, ids in token_ids.items()}\n",
    "\n",
    "print(token_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Calculate context vectors for all validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Contexts: 100%|██████████| 3250/3250 [00:37<00:00, 86.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16225, 768])\n",
      "16225\n"
     ]
    }
   ],
   "source": [
    "val_context_vectors = torch.empty(0, 768).to(device)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "for example in tqdm(dataset['validation'], desc='Collecting Contexts'):\n",
    "    input_ids = torch.tensor(example['input_ids'])\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    target_mask = torch.tensor(example['target_mask'])\n",
    "\n",
    "    embedding_aggregate = get_context_vectors(input_ids, attention_mask, target_mask)\n",
    "    val_context_vectors = torch.cat((val_context_vectors, embedding_aggregate))\n",
    "\n",
    "print(val_context_vectors.shape)\n",
    "print(sum(sum(x[0]) for x in dataset['validation']['target_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify cluster allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n"
     ]
    }
   ],
   "source": [
    "val_cluster_ids = kmeans_predict(val_context_vectors, cluster_centres, distance=distance, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign cluster ids to each token in the dataset and obtain a single cluster id for items with multiple, i.e. words composed of sub-word level tokens, by taking the mode or majority vote. Using `max(set(lst), key=lst.count))` for majority voting yields the most frequent list item or, in the case that there is a tie for most frequent, the item with the lowest value:\n",
    "\n",
    "```python\n",
    "a = [1, 1, 1, 0, 0]\n",
    "b = [1, 1, 0, 0]\n",
    "max(set(a), key=a.count)\n",
    "# >> 1\n",
    "max(set(b), key=b.count)\n",
    "# >> 0\n",
    "```\n",
    "Another implementation may find that the head sub-word token is more informative.\n",
    "\n",
    "Also collect the ground truth NER tag for each entity for comparison. `Note`: The cluster id numbers will not correspond to the NER tag numbers. They are two sets of labels that need to be mapped to eachother in some way.\n",
    "\n",
    "Finally, find which mapping of cluster ids to NER ids yields the best f-1 performance. `Note`: This calculation does not consider entities that weren't tagged by using PROPN. This is to control for the initial entity extraction task which will be reviewed at another stage. This is to just evaluate whether this style of clustering is suitable for identifying entity classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(cluster_ids, split, classes, subtoken_aggregation=False):\n",
    "    cluster_ids = cluster_ids.copy()\n",
    "    sample_predictions = []\n",
    "\n",
    "    for example in tqdm(split):\n",
    "        token_ids = {f'{i}-{token}': {\n",
    "            'ids': [i] if subtoken_aggregation else tokenizer.encode(token)[1:-1],\n",
    "            'ner_tag': ner_tag\n",
    "        } for i, (token, ner_tag) in enumerate(zip(example['tokens'], example['ner_tags'])) if ner_tag != 0}\n",
    "\n",
    "        token_predictions = {token: {\n",
    "            'cluster_id': max(set((lst := [cluster_ids.pop(0) for _ in range(len(attributes['ids']))])), key=lst.count),\n",
    "            'ner_tag': attributes['ner_tag']\n",
    "        } for token, attributes in token_ids.items()}\n",
    "\n",
    "        sample_predictions.append(token_predictions)\n",
    "\n",
    "    df = pd.DataFrame(data={x: 0 for x in set([x for y in split['ner_tags'] for x in y if x != 0])}, index=classes)\n",
    "\n",
    "    for pred in sample_predictions:\n",
    "        for p in pred.values():\n",
    "            df[p['ner_tag']].loc[p['cluster_id']] += 1\n",
    "\n",
    "    eye = np.eye(len(df.columns), dtype=bool)\n",
    "    total = np.sum(df.to_numpy().flatten())\n",
    "\n",
    "    results = []\n",
    "    for i in range(len(df.columns)):\n",
    "        mapping = list(df[(df * np.roll(eye, i, axis=1)) != 0].stack().index)\n",
    "        accuracy = np.sum((df * np.roll(eye, i, axis=1)).to_numpy().flatten()) / total * 100\n",
    "        results.append({'mapping': mapping, 'accuracy': accuracy})\n",
    "\n",
    "    return results, df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [00:01<00:00, 2071.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'accuracy': 17.59851214692549, 'mapping': [(0, 1), (1, 2), (2, 3), (3, 4)]},\n",
      " {'accuracy': 5.579449029408346, 'mapping': [(0, 2), (1, 3), (2, 4), (3, 1)]},\n",
      " {'accuracy': 30.861327443914917, 'mapping': [(0, 3), (1, 4), (2, 1), (3, 2)]},\n",
      " {'accuracy': 45.96071137975125, 'mapping': [(0, 4), (1, 1), (2, 2), (3, 3)]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_clusters_ids_list = val_cluster_ids.squeeze().tolist()\n",
    "results = evaluate(val_clusters_ids_list, dataset['validation'])\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Aggregations\n",
    "\n",
    "Using a more testable implementation of the work in this notebook, explore variations on the original model to evaluate improvement. First import developed class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_vector_clustering import ContextClustering, CatNLayers, SumNLayers, MeanNLayers, SelectLayerN, AggregateSubtokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the class to the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Contexts: 100%|██████████| 14041/14041 [03:14<00:00, 72.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 13it [00:01,  6.54it/s, center_shift=0.000000, iteration=13, tol=0.000100]  \n"
     ]
    }
   ],
   "source": [
    "X_fit = ContextClustering(random_state=42, n_clusters=3).fit(dataset['train'], aggregation=SumNLayers(-4), subtoken_aggregation=AggregateSubtokens(torch.sum))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Predict the class assignment of the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Contexts: 100%|██████████| 3250/3250 [00:43<00:00, 74.33it/s]\n",
      " 55%|█████▌    | 1803/3250 [00:01<00:01, 1250.04it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m y_hat \u001b[39m=\u001b[39m X_fit\u001b[39m.\u001b[39mpredict(dataset[\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m], threshold\u001b[39m=\u001b[39m\u001b[39m0.5\u001b[39m)\n\u001b[1;32m      2\u001b[0m y_hat_list \u001b[39m=\u001b[39m y_hat\u001b[39m.\u001b[39msqueeze()\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m----> 4\u001b[0m results, df \u001b[39m=\u001b[39m evaluate(y_hat_list, dataset[\u001b[39m'\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m'\u001b[39;49m], classes\u001b[39m=\u001b[39;49m[\u001b[39m-\u001b[39;49mnp\u001b[39m.\u001b[39;49minf, \u001b[39m0\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m])\n\u001b[1;32m      6\u001b[0m pprint(results)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(df)\n",
      "Cell \u001b[0;32mIn [4], line 11\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(cluster_ids, split, classes, subtoken_aggregation)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m tqdm(split):\n\u001b[1;32m      6\u001b[0m     token_ids \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: {\n\u001b[1;32m      7\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m: [i] \u001b[39mif\u001b[39;00m subtoken_aggregation \u001b[39melse\u001b[39;00m tokenizer\u001b[39m.\u001b[39mencode(token)[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m      8\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mner_tag\u001b[39m\u001b[39m'\u001b[39m: ner_tag\n\u001b[1;32m      9\u001b[0m     } \u001b[39mfor\u001b[39;00m i, (token, ner_tag) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(example[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m], example[\u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m])) \u001b[39mif\u001b[39;00m ner_tag \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m}\n\u001b[0;32m---> 11\u001b[0m     token_predictions \u001b[39m=\u001b[39m {token: {\n\u001b[1;32m     12\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcluster_id\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mmax\u001b[39m(\u001b[39mset\u001b[39m((lst \u001b[39m:=\u001b[39m [cluster_ids\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(attributes[\u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m]))])), key\u001b[39m=\u001b[39mlst\u001b[39m.\u001b[39mcount),\n\u001b[1;32m     13\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mner_tag\u001b[39m\u001b[39m'\u001b[39m: attributes[\u001b[39m'\u001b[39m\u001b[39mner_tag\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m     } \u001b[39mfor\u001b[39;00m token, attributes \u001b[39min\u001b[39;00m token_ids\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m     sample_predictions\u001b[39m.\u001b[39mappend(token_predictions)\n\u001b[1;32m     18\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39m{x: \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m([x \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m split[\u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m y \u001b[39mif\u001b[39;00m x \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m])}, index\u001b[39m=\u001b[39mclasses)\n",
      "Cell \u001b[0;32mIn [4], line 12\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m tqdm(split):\n\u001b[1;32m      6\u001b[0m     token_ids \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: {\n\u001b[1;32m      7\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m: [i] \u001b[39mif\u001b[39;00m subtoken_aggregation \u001b[39melse\u001b[39;00m tokenizer\u001b[39m.\u001b[39mencode(token)[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m      8\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mner_tag\u001b[39m\u001b[39m'\u001b[39m: ner_tag\n\u001b[1;32m      9\u001b[0m     } \u001b[39mfor\u001b[39;00m i, (token, ner_tag) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(example[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m], example[\u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m])) \u001b[39mif\u001b[39;00m ner_tag \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m}\n\u001b[1;32m     11\u001b[0m     token_predictions \u001b[39m=\u001b[39m {token: {\n\u001b[0;32m---> 12\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcluster_id\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mmax\u001b[39m(\u001b[39mset\u001b[39m((lst \u001b[39m:=\u001b[39m [cluster_ids\u001b[39m.\u001b[39mpop(\u001b[39m0\u001b[39m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(attributes[\u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m]))])), key\u001b[39m=\u001b[39mlst\u001b[39m.\u001b[39mcount),\n\u001b[1;32m     13\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mner_tag\u001b[39m\u001b[39m'\u001b[39m: attributes[\u001b[39m'\u001b[39m\u001b[39mner_tag\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m     } \u001b[39mfor\u001b[39;00m token, attributes \u001b[39min\u001b[39;00m token_ids\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m     sample_predictions\u001b[39m.\u001b[39mappend(token_predictions)\n\u001b[1;32m     18\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39m{x: \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m([x \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m split[\u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m y \u001b[39mif\u001b[39;00m x \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m])}, index\u001b[39m=\u001b[39mclasses)\n",
      "Cell \u001b[0;32mIn [4], line 12\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m example \u001b[39min\u001b[39;00m tqdm(split):\n\u001b[1;32m      6\u001b[0m     token_ids \u001b[39m=\u001b[39m {\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m-\u001b[39m\u001b[39m{\u001b[39;00mtoken\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m: {\n\u001b[1;32m      7\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m: [i] \u001b[39mif\u001b[39;00m subtoken_aggregation \u001b[39melse\u001b[39;00m tokenizer\u001b[39m.\u001b[39mencode(token)[\u001b[39m1\u001b[39m:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m],\n\u001b[1;32m      8\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mner_tag\u001b[39m\u001b[39m'\u001b[39m: ner_tag\n\u001b[1;32m      9\u001b[0m     } \u001b[39mfor\u001b[39;00m i, (token, ner_tag) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mzip\u001b[39m(example[\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m], example[\u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m])) \u001b[39mif\u001b[39;00m ner_tag \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m}\n\u001b[1;32m     11\u001b[0m     token_predictions \u001b[39m=\u001b[39m {token: {\n\u001b[0;32m---> 12\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mcluster_id\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mmax\u001b[39m(\u001b[39mset\u001b[39m((lst \u001b[39m:=\u001b[39m [cluster_ids\u001b[39m.\u001b[39;49mpop(\u001b[39m0\u001b[39;49m) \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(attributes[\u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m]))])), key\u001b[39m=\u001b[39mlst\u001b[39m.\u001b[39mcount),\n\u001b[1;32m     13\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mner_tag\u001b[39m\u001b[39m'\u001b[39m: attributes[\u001b[39m'\u001b[39m\u001b[39mner_tag\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     14\u001b[0m     } \u001b[39mfor\u001b[39;00m token, attributes \u001b[39min\u001b[39;00m token_ids\u001b[39m.\u001b[39mitems()}\n\u001b[1;32m     16\u001b[0m     sample_predictions\u001b[39m.\u001b[39mappend(token_predictions)\n\u001b[1;32m     18\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(data\u001b[39m=\u001b[39m{x: \u001b[39m0\u001b[39m \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mset\u001b[39m([x \u001b[39mfor\u001b[39;00m y \u001b[39min\u001b[39;00m split[\u001b[39m'\u001b[39m\u001b[39mner_tags\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m y \u001b[39mif\u001b[39;00m x \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m])}, index\u001b[39m=\u001b[39mclasses)\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "y_hat = X_fit.predict(dataset['validation'], threshold=0.5)\n",
    "y_hat_list = y_hat.squeeze().tolist()\n",
    "\n",
    "results, df = evaluate(y_hat_list, dataset['validation'], classes=[-np.inf, 0, 1, 2], subtoken_aggregation=True)\n",
    "\n",
    "pprint(results)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('gatenlp_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5e1f5b7c86f3a3638279a01f4f037ad1bf683427fdb20ed40b35537abd7abab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
