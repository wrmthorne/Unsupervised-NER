{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "from kmeans_pytorch import kmeans, kmeans_predict\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from datasets import load_from_disk\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-cased'\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(\"bert-base-cased\", output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "The CoNLL2003 dataset is first used to evaluate performance. The ground truth ner tags are used only select named entities to allow the class allocation to be tested independently by assumung perfect extraction. Later work will be conducted into identifying which entities should be labelled. NER tags are mapped to a non-BIO based scheme to reduce the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found cached dataset conll2003 (/home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98)\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 675.77it/s]\n",
      "Loading cached processed dataset at /home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-320df3d2a03ea85a.arrow\n",
      "Loading cached processed dataset at /home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-0a73fd1c3c751351.arrow\n",
      "Loading cached processed dataset at /home/william/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/9a4d16a94f8674ba3466315300359b0acd891b68b6c8743ddf60b9c702adce98/cache-9f967a16c9b36547.arrow\n",
      "{'tokens': ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7], 'ner_tags': [2, 0, 4, 0, 0, 0, 4, 0, 0], 'input_ids': [[101, 7270, 22961, 1528, 1840, 1106, 21423, 1418, 2495, 12913, 119, 102]], 'target_mask': [[0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "!python format_conll2003.py --bert_model $model_name\n",
    "\n",
    "with open('./data/CoNLL2003/tag_mappings.json') as file:\n",
    "    tag_mappings = json.load(file)\n",
    "\n",
    "dataset = load_from_disk('./data/CoNLL2003')\n",
    "\n",
    "print(dataset['train'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "Define a method to extract the context vectors of the extracted entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model.to(device)\n",
    "def get_context_vectors(input_ids, attention_mask, target_mask):\n",
    "    # Ignore gradient to improve performance\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids.to(device), attention_mask.to(device))\n",
    "        hidden_states = outputs.hidden_states\n",
    "\n",
    "    # Extract model embeddings layer activations\n",
    "    token_embeddings = torch.stack(hidden_states, dim=0)\n",
    "\n",
    "    # Remove batches dimension\n",
    "    token_embeddings = torch.squeeze(token_embeddings, dim=1)\n",
    "\n",
    "    # Swap layer and token dimensions\n",
    "    token_embeddings = token_embeddings.permute(1, 0, 2)\n",
    "    \n",
    "    # Identify indices within encoded text to calculate context embeddings\n",
    "    target_indices = target_mask.to(device).nonzero()\n",
    "\n",
    "    # Use the sum of the last 4 embedding layers as an aggregation of context for the selected indices\n",
    "    stacked_token_embeddings = token_embeddings.repeat(torch.sum(target_mask), 1, 1, 1)\n",
    "    embedding_aggregate = torch.sum(stacked_token_embeddings[torch.arange(0, torch.sum(target_mask)), target_indices[:, 1], -4:], dim=1)\n",
    "\n",
    "    return embedding_aggregate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract all of the context vectors using the defined method on the training split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Contexts: 100%|██████████| 14041/14041 [02:41<00:00, 87.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([65030, 768])\n",
      "65030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "context_vectors = torch.empty(0, 768).to(device)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "for example in tqdm(dataset['train'], desc='Collecting Contexts'):\n",
    "    input_ids = torch.tensor(example['input_ids'])\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    target_mask = torch.tensor(example['target_mask'])\n",
    "\n",
    "    embedding_aggregate = get_context_vectors(input_ids, attention_mask, target_mask)\n",
    "    context_vectors = torch.cat((context_vectors, embedding_aggregate))\n",
    "\n",
    "print(context_vectors.shape)\n",
    "print(sum(sum(x[0]) for x in dataset['train']['target_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster\n",
    "\n",
    "Cluster context vectors using kmeans with $k = 4$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running k-means on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[running kmeans]: 39it [00:11,  3.33it/s, center_shift=0.000000, iteration=39, tol=0.000100]  \n"
     ]
    }
   ],
   "source": [
    "# Seeds for reproducing cluster centres\n",
    "torch.random.seed = 42\n",
    "torch.cuda.seed = 42\n",
    "np.random.seed(42)\n",
    "\n",
    "distance = 'cosine' # 'euclidean'\n",
    "\n",
    "cluster_ids, cluster_centres = kmeans(context_vectors, 4, distance=distance, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually evaluate clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n",
      "{'European': [3], 'Commission': [3], 'German': [3], 'British': [3]}\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "example = dataset['train'][i]\n",
    "\n",
    "input_ids = torch.tensor(example['input_ids'])\n",
    "attention_mask = torch.ones_like(input_ids)\n",
    "target_mask = torch.tensor(example['target_mask'])\n",
    "\n",
    "test_context_vectors = get_context_vectors(input_ids, attention_mask, target_mask)\n",
    "\n",
    "if len(test_context_vectors) == 1:\n",
    "    test_context_vectors = test_context_vectors.repeat(2, 1)\n",
    "\n",
    "test_cluster_ids = kmeans_predict(test_context_vectors, cluster_centres, distance=distance, device=device)\n",
    "test_clusters_ids_list = test_cluster_ids.squeeze().tolist()\n",
    "\n",
    "token_ids = {token: tokenizer.encode(token)[1:-1] for token, ner_tag in zip(example['tokens'], example['ner_tags']) if ner_tag != 0}\n",
    "token_predictions = {token: [test_clusters_ids_list.pop(0) for _ in range(len(ids))] for token, ids in token_ids.items()}\n",
    "\n",
    "print(token_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Calculate context vectors for all validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Contexts: 100%|██████████| 3250/3250 [00:37<00:00, 86.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16225, 768])\n",
      "16225\n"
     ]
    }
   ],
   "source": [
    "val_context_vectors = torch.empty(0, 768).to(device)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "for example in tqdm(dataset['validation'], desc='Collecting Contexts'):\n",
    "    input_ids = torch.tensor(example['input_ids'])\n",
    "    attention_mask = torch.ones_like(input_ids)\n",
    "    target_mask = torch.tensor(example['target_mask'])\n",
    "\n",
    "    embedding_aggregate = get_context_vectors(input_ids, attention_mask, target_mask)\n",
    "    val_context_vectors = torch.cat((val_context_vectors, embedding_aggregate))\n",
    "\n",
    "print(val_context_vectors.shape)\n",
    "print(sum(sum(x[0]) for x in dataset['validation']['target_mask']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify cluster allocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n"
     ]
    }
   ],
   "source": [
    "val_cluster_ids = kmeans_predict(val_context_vectors, cluster_centres, distance=distance, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign cluster ids to each token in the dataset and obtain a single cluster id for items with multiple, i.e. words composed of sub-word level tokens, by taking the mode or majority vote. Using `max(set(lst), key=lst.count))` for majority voting yields the most frequent list item or, in the case that there is a tie for most frequent, the item with the lowest value:\n",
    "\n",
    "```python\n",
    "a = [1, 1, 1, 0, 0]\n",
    "b = [1, 1, 0, 0]\n",
    "max(set(a), key=a.count)\n",
    "# >> 1\n",
    "max(set(b), key=b.count)\n",
    "# >> 0\n",
    "```\n",
    "Another implementation may find that the head sub-word token is more informative.\n",
    "\n",
    "Also collect the ground truth NER tag for each entity for comparison. `Note`: The cluster id numbers will not correspond to the NER tag numbers. They are two sets of labels that need to be mapped to eachother in some way.\n",
    "\n",
    "Finally, find which mapping of cluster ids to NER ids yields the best f-1 performance. `Note`: This calculation does not consider entities that weren't tagged by using PROPN. This is to control for the initial entity extraction task which will be reviewed at another stage. This is to just evaluate whether this style of clustering is suitable for identifying entity classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(cluster_ids, split):\n",
    "    sample_predictions = []\n",
    "\n",
    "    for example in tqdm(split):\n",
    "        token_ids = {f'{i}-{token}': {\n",
    "            'ids': tokenizer.encode(token)[1:-1],\n",
    "            'ner_tag': ner_tag\n",
    "        } for i, (token, ner_tag) in enumerate(zip(example['tokens'], example['ner_tags'])) if ner_tag != 0}\n",
    "\n",
    "        token_predictions = {token: {\n",
    "            'cluster_id': max(set((lst := [cluster_ids.pop(0) for _ in range(len(attributes['ids']))])), key=lst.count),\n",
    "            'ner_tag': attributes['ner_tag']\n",
    "        } for token, attributes in token_ids.items()}\n",
    "\n",
    "        sample_predictions.append(token_predictions)\n",
    "\n",
    "    mapping_freqs = Counter([(x_i['cluster_id'], x_i['ner_tag']) for x in sample_predictions for x_i in x.values()])\n",
    "    p = defaultdict(dict)\n",
    "\n",
    "    for k, v in mapping_freqs.items():\n",
    "        p[k[0]].update({k[1]: v})\n",
    "\n",
    "    df = pd.DataFrame.from_dict(data=p, orient='index').sort_index()\n",
    "    df = df.reindex(sorted(df.columns), axis=1)\n",
    "\n",
    "    eye = np.eye(len(df.columns), dtype=bool)\n",
    "    total = np.sum(df.to_numpy().flatten())\n",
    "\n",
    "    results = []\n",
    "    for i in range(len(df.columns)):\n",
    "        mapping = list(df[(df * np.roll(eye, i, axis=1)) != 0].stack().index)\n",
    "        accuracy = np.sum((df * np.roll(eye, i, axis=1)).to_numpy().flatten()) / total * 100\n",
    "        results.append({'mapping': mapping, 'accuracy': accuracy})\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [00:01<00:00, 2071.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'accuracy': 17.59851214692549, 'mapping': [(0, 1), (1, 2), (2, 3), (3, 4)]},\n",
      " {'accuracy': 5.579449029408346, 'mapping': [(0, 2), (1, 3), (2, 4), (3, 1)]},\n",
      " {'accuracy': 30.861327443914917, 'mapping': [(0, 3), (1, 4), (2, 1), (3, 2)]},\n",
      " {'accuracy': 45.96071137975125, 'mapping': [(0, 4), (1, 1), (2, 2), (3, 3)]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "val_clusters_ids_list = val_cluster_ids.squeeze().tolist()\n",
    "results = evaluate(val_clusters_ids_list, dataset['validation'])\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different Aggregations\n",
    "\n",
    "Using a more testable implementation of the work in this notebook, explore variations on the original model to evaluate improvement. First import developed class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from context_vector_clustering import ContextClustering, CatNLayers, SumNLayers, MeanNLayers, SelectLayerN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the class to the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to create tensor with negative dimension -3072: [0, -3072]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_fit \u001b[39m=\u001b[39m ContextClustering(random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(dataset[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m], aggregation\u001b[39m=\u001b[39;49mCatNLayers(\u001b[39m-\u001b[39;49m\u001b[39m4\u001b[39;49m))\n",
      "File \u001b[0;32m~/Documents/university/SSLNER/context_vector_clustering.py:198\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(self, X, distance, masked_targets, aggregation)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, distance\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m, masked_targets\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, aggregation\u001b[39m=\u001b[39mSumNLayers(\u001b[39m4\u001b[39m)):\n\u001b[1;32m    192\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    193\u001b[0m \u001b[39m    Finds all the relevant context vectors in the data and clusters them using KMeans\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \n\u001b[1;32m    195\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m        X: Huggingface Dataset split with columns ['input_ids', 'target_mask']\u001b[39;00m\n\u001b[1;32m    197\u001b[0m \u001b[39m        distance: Distance metric to use from ['cosine', 'euclidean']\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[39m        masked_targets: Attention masking strategy. True uses the inverse of the target mask\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m            as the attention mask\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[39m        aggregation: callable for aggregation strategy of context vectors\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39m        self: Fitted estimator\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m    205\u001b[0m     \u001b[39massert\u001b[39;00m distance \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mcosine\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m    207\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistance \u001b[39m=\u001b[39m distance\n",
      "File \u001b[0;32m~/Documents/university/SSLNER/context_vector_clustering.py:162\u001b[0m, in \u001b[0;36m_collect_contexts\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_collect_contexts\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    159\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m    Iterates over all rows in the dataset split and obtains the context vectors of target\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    tokens\u001b[39;00m\n\u001b[0;32m--> 162\u001b[0m \u001b[39m    \u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39m        X: Huggingface Dataset split with columns ['input_ids', 'target_mask']\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \n\u001b[1;32m    166\u001b[0m \u001b[39m    Returns:\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m        tensor of shape (n, m*768) where n is the total sum of 1s that appear in the target_mask\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[39m            column for m > 1 if concatenation of 2+ layers is used as as aggregation, else m = 1\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[1;32m    170\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregation, CatNLayers):\n\u001b[1;32m    171\u001b[0m         context_dim \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maggregation\u001b[39m.\u001b[39mn \u001b[39m*\u001b[39m \u001b[39m768\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to create tensor with negative dimension -3072: [0, -3072]"
     ]
    }
   ],
   "source": [
    "X_fit = ContextClustering(random_state=42).fit(dataset['train'], aggregation=CatNLayers(-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "\n",
    "Predict the class assignment of the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Collecting Contexts: 100%|██████████| 3250/3250 [00:36<00:00, 87.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on cuda..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3250/3250 [00:01<00:00, 2231.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'accuracy': 14.576310589329303, 'mapping': [(0, 1), (1, 2), (2, 3), (3, 4)]},\n",
      " {'accuracy': 7.509008485412065, 'mapping': [(0, 2), (1, 3), (2, 4), (3, 1)]},\n",
      " {'accuracy': 53.89980239451354, 'mapping': [(0, 3), (1, 4), (2, 1), (3, 2)]},\n",
      " {'accuracy': 24.01487853074509, 'mapping': [(0, 4), (1, 1), (2, 2), (3, 3)]}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat = X_fit.predict(dataset['validation'])\n",
    "y_hat_list = y_hat.squeeze().tolist()\n",
    "\n",
    "results = evaluate(y_hat_list, dataset['validation'])\n",
    "pprint(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "1) Subword-token aggregation when calculating context vectors\n",
    "2) Getting generic context vector instead of word based context vector\n",
    "3) Multi-word token aggregation for single entity context vector\n",
    "4) Different aggregation techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('gatenlp_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b5e1f5b7c86f3a3638279a01f4f037ad1bf683427fdb20ed40b35537abd7abab"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
